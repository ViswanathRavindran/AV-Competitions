{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from time import time\n",
    "import gc\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    try:\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``'], tokens))\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train data is  (920871, 19)  Test data is  (773858, 17)  Valid data is (102320, 19)\n",
      "Time taken for importing the data is:  41.4264178276062  secs\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "# train = pd.read_csv(path+'train.csv',)\n",
    "# test = pd.read_csv(path+'test.csv')\n",
    "# camp = pd.read_csv(path+'campaign_data.csv')\n",
    "\n",
    "# print('New variable creations')\n",
    "# train['hour'] = pd.to_datetime(train.send_date).dt.hour.astype('uint8')\n",
    "# train['day'] = pd.to_datetime(train.send_date).dt.day.astype('uint8')\n",
    "# train['dow'] = pd.to_datetime(train.send_date).dt.dayofweek.astype('uint8')\n",
    "\n",
    "# test['hour'] = pd.to_datetime(test.send_date).dt.hour.astype('uint8')\n",
    "# test['day'] = pd.to_datetime(test.send_date).dt.day.astype('uint8')\n",
    "# test['dow'] = pd.to_datetime(test.send_date).dt.dayofweek.astype('uint8')\n",
    "\n",
    "# camp['sub_tokens'] = camp['subject'].map(tokenizer)\n",
    "# camp['email_tokens'] = camp['email_body'].map(tokenizer)\n",
    "# train_df = pd.merge(train, camp, on='campaign_id')\n",
    "# test_df = pd.merge(test, camp, on='campaign_id')\n",
    "\n",
    "train_df = pd.read_csv(path+'train_df.csv')\n",
    "test_df = pd.read_csv(path+'test_df.csv')\n",
    "\n",
    "\n",
    "train_df, dev_df = train_test_split(train_df, random_state=42, test_size=0.10)\n",
    "\n",
    "print('Shape of the train data is ',train_df.shape,' Test data is ',test_df.shape,' Valid data is',dev_df.shape)\n",
    "print('Time taken for importing the data is: ', time()-start, ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming of the new date variables created\n"
     ]
    }
   ],
   "source": [
    "print('Renaming of the new date variables created')\n",
    "train_df['dow'] = train_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "train_df['hour'] = train_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "train_df['day'] = train_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)\n",
    "\n",
    "\n",
    "dev_df['dow'] = dev_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "dev_df['hour'] = dev_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "dev_df['day'] = dev_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)\n",
    "\n",
    "\n",
    "test_df['dow'] = test_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "test_df['hour'] = test_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "test_df['day'] = test_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping variables creation for train_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Grouping variables creation for train_data')\n",
    "# ip - user_id,  Channel - campaign_id\n",
    "\n",
    "# Campaigns per communication\n",
    "gp = train_df[['user_id','campaign_id']].groupby(by=['communication_type'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'totcom'})\n",
    "train_df = train_df.merge(gp, on=['communication_type'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "# open count per communication\n",
    "gp = train_df[['communication_type','is_open']].groupby(by=['communication_type'])[['is_open']].sum().reset_index().rename(index=str, columns={'is_open': 'opncom'})\n",
    "train_df = train_df.merge(gp, on=['communication_type'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "# Click count per communication\n",
    "gp = train_df[['communication_type','is_click']].groupby(by=['communication_type'])[['is_click']].sum().reset_index().rename(index=str, columns={'is_click': 'clkcom'})\n",
    "train_df = train_df.merge(gp, on=['communication_type'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "gp = train_df[['communication_type','dow','campaign_id']].groupby(by=['communication_type','dow'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'totdow'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "gp = train_df[['communication_type','dow','is_open']].groupby(by=['communication_type','dow'])[['is_open']].sum().reset_index().rename(index=str, columns={'is_open': 'opndow'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "gp = train_df[['communication_type','dow','is_click']].groupby(by=['communication_type','dow'])[['is_click']].sum().reset_index().rename(index=str, columns={'is_click': 'clkdow'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','dow'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "gp = train_df[['communication_type','day','campaign_id']].groupby(by=['communication_type','day'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'totday'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "gp = train_df[['communication_type','day','is_open']].groupby(by=['communication_type','day'])[['is_open']].sum().reset_index().rename(index=str, columns={'is_open': 'opnday'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "gp = train_df[['communication_type','day','is_click']].groupby(by=['communication_type','day'])[['is_click']].sum().reset_index().rename(index=str, columns={'is_click': 'clkday'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','day'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "gp = train_df[['communication_type','hour','campaign_id']].groupby(by=['communication_type','hour'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'tothour'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "gp = train_df[['communication_type','hour','is_open']].groupby(by=['communication_type','hour'])[['is_open']].sum().reset_index().rename(index=str, columns={'is_open': 'opnhour'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "gp = train_df[['communication_type','hour','is_click']].groupby(by=['communication_type','hour'])[['is_click']].sum().reset_index().rename(index=str, columns={'is_click': 'clkhour'})\n",
    "train_df = train_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "dev_df = dev_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "test_df = test_df.merge(gp, on=['communication_type','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for Training variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for Training variables\")\n",
    "train_df['totcom'] = train_df['totcom'].fillna(0).astype('uint16')\n",
    "train_df['opncom'] = train_df['opncom'].fillna(0).astype('uint16')\n",
    "train_df['clkcom'] = train_df['clkcom'].fillna(0).astype('uint16')\n",
    "train_df['opncomrate'] = ((train_df['opncom']/train_df['totcom']).replace(np.inf, 0))\n",
    "train_df['clkcomrate'] = ((train_df['clkcom']/train_df['totcom']).replace(np.inf, 0))\n",
    "\n",
    "train_df['totdow'] = train_df['totdow'].fillna(0).astype('uint16')\n",
    "train_df['opndow'] = train_df['opndow'].fillna(0).astype('uint16')\n",
    "train_df['clkdow'] = train_df['clkdow'].fillna(0).astype('uint16')\n",
    "train_df['opndowrate'] = ((train_df['opndow']/train_df['totdow']).replace(np.inf, 0))\n",
    "train_df['clkdowrate'] = ((train_df['clkdow']/train_df['totdow']).replace(np.inf, 0))\n",
    "\n",
    "train_df['totday'] = train_df['totday'].fillna(0).astype('uint16')\n",
    "train_df['opnday'] = train_df['opnday'].fillna(0).astype('uint16')\n",
    "train_df['clkday'] = train_df['clkday'].fillna(0).astype('uint16')\n",
    "train_df['opndayrate'] = ((train_df['opnday']/train_df['totday']).replace(np.inf, 0))\n",
    "train_df['clkdayrate'] = ((train_df['clkday']/train_df['totday']).replace(np.inf, 0))\n",
    "\n",
    "train_df['tothour'] = train_df['tothour'].fillna(0).astype('uint16')\n",
    "train_df['opnhour'] = train_df['opnhour'].fillna(0).astype('uint16')\n",
    "train_df['clkhour'] = train_df['clkhour'].fillna(0).astype('uint16')\n",
    "train_df['opnhourrate'] = ((train_df['opnhour']/train_df['tothour']).replace(np.inf, 0))\n",
    "train_df['clkhourrate'] = ((train_df['clkhour']/train_df['tothour']).replace(np.inf, 0))\n",
    "\n",
    "del train_df['totcom'],train_df['opncom'],train_df['clkcom'],train_df['totdow'],train_df['opndow'],train_df['clkdow']\n",
    "del train_df['totday'],train_df['opnday'],train_df['clkday'],train_df['tothour'],train_df['opnhour'],train_df['clkhour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['totcom'] = dev_df['totcom'].fillna(0).astype('uint16')\n",
    "dev_df['opncom'] = dev_df['opncom'].fillna(0).astype('uint16')\n",
    "dev_df['clkcom'] = dev_df['clkcom'].fillna(0).astype('uint16')\n",
    "dev_df['opncomrate'] = ((dev_df['opncom']/dev_df['totcom']).replace(np.inf, 0))\n",
    "dev_df['clkcomrate'] = ((dev_df['clkcom']/dev_df['totcom']).replace(np.inf, 0))\n",
    "\n",
    "dev_df['totdow'] = dev_df['totdow'].fillna(0).astype('uint16')\n",
    "dev_df['opndow'] = dev_df['opndow'].fillna(0).astype('uint16')\n",
    "dev_df['clkdow'] = dev_df['clkdow'].fillna(0).astype('uint16')\n",
    "dev_df['opndowrate'] = ((dev_df['opndow']/dev_df['totdow']).replace(np.inf, 0))\n",
    "dev_df['clkdowrate'] = ((dev_df['clkdow']/dev_df['totdow']).replace(np.inf, 0))\n",
    "\n",
    "dev_df['totday'] = dev_df['totday'].fillna(0).astype('uint16')\n",
    "dev_df['opnday'] = dev_df['opnday'].fillna(0).astype('uint16')\n",
    "dev_df['clkday'] = dev_df['clkday'].fillna(0).astype('uint16')\n",
    "dev_df['opndayrate'] = ((dev_df['opnday']/dev_df['totday']).replace(np.inf, 0))\n",
    "dev_df['clkdayrate'] = ((dev_df['clkday']/dev_df['totday']).replace(np.inf, 0))\n",
    "\n",
    "dev_df['tothour'] = dev_df['tothour'].fillna(0).astype('uint16')\n",
    "dev_df['opnhour'] = dev_df['opnhour'].fillna(0).astype('uint16')\n",
    "dev_df['clkhour'] = dev_df['clkhour'].fillna(0).astype('uint16')\n",
    "dev_df['opnhourrate'] = ((dev_df['opnhour']/dev_df['tothour']).replace(np.inf, 0))\n",
    "dev_df['clkhourrate'] = ((dev_df['clkhour']/dev_df['tothour']).replace(np.inf, 0))\n",
    "\n",
    "del dev_df['totcom'],dev_df['opncom'],dev_df['clkcom'],dev_df['totdow'],dev_df['opndow'],dev_df['clkdow']\n",
    "del dev_df['totday'],dev_df['opnday'],dev_df['clkday'],dev_df['tothour'],dev_df['opnhour'],dev_df['clkhour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['totcom'] = test_df['totcom'].fillna(0).astype('uint16')\n",
    "test_df['opncom'] = test_df['opncom'].fillna(0).astype('uint16')\n",
    "test_df['clkcom'] = test_df['clkcom'].fillna(0).astype('uint16')\n",
    "test_df['opncomrate'] = ((test_df['opncom']/test_df['totcom']).replace(np.inf, 0))\n",
    "test_df['clkcomrate'] = ((test_df['clkcom']/test_df['totcom']).replace(np.inf, 0))\n",
    "\n",
    "test_df['totdow'] = test_df['totdow'].fillna(0).astype('uint16')\n",
    "test_df['opndow'] = test_df['opndow'].fillna(0).astype('uint16')\n",
    "test_df['clkdow'] = test_df['clkdow'].fillna(0).astype('uint16')\n",
    "test_df['opndowrate'] = ((test_df['opndow']/test_df['totdow']).replace(np.inf, 0))\n",
    "test_df['clkdowrate'] = ((test_df['clkdow']/test_df['totdow']).replace(np.inf, 0))\n",
    "\n",
    "test_df['totday'] = test_df['totday'].fillna(0).astype('uint16')\n",
    "test_df['opnday'] = test_df['opnday'].fillna(0).astype('uint16')\n",
    "test_df['clkday'] = test_df['clkday'].fillna(0).astype('uint16')\n",
    "test_df['opndayrate'] = ((test_df['opnday']/test_df['totday']).replace(np.inf, 0))\n",
    "test_df['clkdayrate'] = ((test_df['clkday']/test_df['totday']).replace(np.inf, 0))\n",
    "\n",
    "test_df['tothour'] = test_df['tothour'].fillna(0).astype('uint16')\n",
    "test_df['opnhour'] = test_df['opnhour'].fillna(0).astype('uint16')\n",
    "test_df['clkhour'] = test_df['clkhour'].fillna(0).astype('uint16')\n",
    "test_df['opnhourrate'] = ((test_df['opnhour']/test_df['tothour']).replace(np.inf, 0))\n",
    "test_df['clkhourrate'] = ((test_df['clkhour']/test_df['tothour']).replace(np.inf, 0))\n",
    "\n",
    "del test_df['totcom'],test_df['opncom'],test_df['clkcom'],test_df['totdow'],test_df['opndow'],test_df['clkdow']\n",
    "del test_df['totday'],test_df['opnday'],test_df['clkday'],test_df['tothour'],test_df['opnhour'],test_df['clkhour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the data\n",
    "y_train = (train_df[['is_open', 'is_click']])\n",
    "y_dev = (dev_df[['is_open', 'is_click']])\n",
    "test_id = test_df['id'].values\n",
    "del test_df['id'],train_df['is_open'],train_df['is_click'],dev_df['is_open'],dev_df['is_click']\n",
    "\n",
    "n_trains = train_df.shape[0]\n",
    "n_devs = dev_df.shape[0]\n",
    "n_tests = test_df.shape[0]\n",
    "\n",
    "full_df = pd.concat([train_df,dev_df,test_df])\n",
    "del train_df,dev_df,test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming of the new date variables created\n",
      "Time taken for renaming date variable:  1.6065678596496582  secs\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print('Renaming of the new date variables created')\n",
    "full_df['dow'] = full_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "full_df['hour'] = full_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "full_df['day'] = full_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)\n",
    "\n",
    "print('Time taken for renaming date variable: ', time()-start, ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer for subject......................\n",
      "Time taken for TfidfVectorizer for subject:  41.483054876327515  secs with shape:  (1797049, 435)\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer for subject\n",
    "start=time()\n",
    "print(\"TfidfVectorizer for subject......................\")\n",
    "TFIDF1 = TfidfVectorizer(max_features = 1000, ngram_range = (1,2), stop_words = \"english\")\n",
    "X_sub = TFIDF1.fit_transform(full_df[\"sub_tokens\"])\n",
    "print('Time taken for TfidfVectorizer for subject: ', time()-start, ' secs with shape: ', X_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer for email Description......................\n",
      "Time taken for TfidfVectorizer for email Description:  523.4367690086365  secs with shape:  (1797049, 1000)\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer for email\n",
    "start=time()\n",
    "print(\"TfidfVectorizer for email Description......................\")\n",
    "TFIDF2 = TfidfVectorizer(max_features = 100000, ngram_range = (1,2), stop_words = \"english\")\n",
    "X_email = TFIDF1.fit_transform(full_df[\"email_tokens\"])\n",
    "print('Time taken for TfidfVectorizer for email Description: ', time()-start, ' secs with shape: ', X_email.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Encoders for communication and day, time..................\n",
      "Time taken for Dummification is:  25.771622896194458  secs with shape:  (1797049, 27)\n"
     ]
    }
   ],
   "source": [
    "# Dummification of the categorical variables in the data\n",
    "start=time()\n",
    "print(\"Dummy Encoders for communication and day, time..................\")\n",
    "X_dummy = scipy.sparse.csr_matrix(pd.get_dummies(full_df[['communication_type','day','dow','hour']], sparse = True).values)\n",
    "print('Time taken for Dummification is: ', time()-start,' secs with shape: ',X_dummy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign_id', 'clkcomrate', 'clkdayrate', 'clkdowrate', 'clkhourrate',\n",
       "       'communication_type', 'day', 'dow', 'email_body', 'email_tokens',\n",
       "       'email_url', 'hour', 'id', 'no_of_images', 'no_of_internal_links',\n",
       "       'no_of_sections', 'opncomrate', 'opndayrate', 'opndowrate',\n",
       "       'opnhourrate', 'send_date', 'sub_tokens', 'subject', 'total_links',\n",
       "       'user_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering the email property continous variables..................\n",
      "Time taken for Gathering the email property continous variables is:  2.3386292457580566  secs with shape:  (1797049, 8)\n"
     ]
    }
   ],
   "source": [
    "# Gathering the continous variables\n",
    "start=time()\n",
    "print(\"Gathering the email property continous variables..................\")\n",
    "X_continous = scipy.sparse.csr_matrix(full_df[['no_of_images','no_of_internal_links','no_of_sections',\n",
    "                                               'total_links','clkcomrate', 'clkdayrate', 'clkdowrate', \n",
    "                                               'clkhourrate']].values)\n",
    "print('Time taken for Gathering the email property continous variables is: ', time()-start,' secs with shape: ',X_continous.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking up the X......................\n",
      "X is ready with shape: (920871, 1470) (102320, 1470) (773858, 1470)\n"
     ]
    }
   ],
   "source": [
    "start=time()\n",
    "print(\"Stacking up the X......................\")\n",
    "X = scipy.sparse.hstack((X_sub,X_email,X_dummy,X_continous)).tocsr()\n",
    "# ,X_counts\n",
    "X_train = X[:n_trains]\n",
    "X_dev = X[n_trains:n_trains+n_devs]\n",
    "X_test = X[n_trains+n_devs:]\n",
    "\n",
    "del X_sub,X_email,X_dummy,X_continous #,full_df\n",
    "#,X_counts\\\n",
    "gc.collect()\n",
    "\n",
    "print('X is ready with shape:',X_train.shape, X_dev.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Modeling now in progress.......................\n",
      "Model complete with accuracy:  0.5\n",
      "Model complete with accuracy:  0.5\n",
      "Time taken for Ridge Modeling is:  188.465106010437  secs\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "y=(y_train['is_click'].values)\n",
    "y_valid= (y_dev['is_click'].values)\n",
    "\n",
    "print ('Ridge Modeling now in progress.......................')\n",
    "model1 = RidgeClassifier(solver = \"saga\", fit_intercept=False, random_state=42, alpha=2)\n",
    "model1.fit(X_train, y)\n",
    "preds1 = model1.predict(X_dev)\n",
    "print('Model complete with accuracy: ', roc_auc_score(y_valid, preds1))\n",
    "\n",
    "model2 = RidgeClassifier(solver=\"sag\", fit_intercept=False, random_state=42, alpha=2)\n",
    "model2.fit(X_train, y)\n",
    "preds2 = model2.predict(X_dev)\n",
    "print('Model complete with accuracy: ', roc_auc_score(y_valid, preds2))\n",
    "print('Time taken for Ridge Modeling is: ', time()-start,' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light GBM Modeling now in progress.......................\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.591433\tvalid_1's auc: 0.593402\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.591433\tvalid_1's auc: 0.593402\n",
      "Prediction for lgb 1 completed. 56.00239896774292  secs\n",
      "Accuracy:  0.5916081094189191\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.591433\tvalid_1's auc: 0.593402\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's auc: 0.591433\tvalid_1's auc: 0.593402\n",
      "Accuracy:  0.5916081094189191\n",
      "Prediction for lgb 2 completed. 66.15966606140137  secs\n"
     ]
    }
   ],
   "source": [
    "print ('Light GBM Modeling now in progress.......................')\n",
    "start = time()\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y, test_size = 0.2, random_state = 42) \n",
    "d_train = lgb.Dataset(train_X, label=train_y)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y)\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "params = {'learning_rate': 0.05, 'objective': 'binary','metric': 'AUC'}\n",
    "\n",
    "params2 = {'learning_rate': 0.04, 'objective': 'binary', 'metric':'AUC'}\n",
    "            \n",
    "model3 = lgb.train(params, train_set=d_train, num_boost_round=2000, valid_sets=watchlist,\n",
    "                    early_stopping_rounds=100, verbose_eval=100)\n",
    "preds3 = model3.predict(X_dev)\n",
    "print('Prediction for lgb 1 completed.', time()-start,  ' secs')\n",
    "print('Accuracy: ', roc_auc_score(y_valid, preds3))\n",
    "\n",
    "start = time()    \n",
    "train_X2, valid_X2, train_y2, valid_y2 = train_test_split(X_train, y, test_size = 0.2, random_state = 42) \n",
    "d_train2 = lgb.Dataset(train_X2, label=train_y2)\n",
    "d_valid2 = lgb.Dataset(valid_X2, label=valid_y2)\n",
    "watchlist2 = [d_train2, d_valid2]\n",
    "\n",
    "model4 = lgb.train(params2, train_set=d_train2, num_boost_round=2000, valid_sets=watchlist2,\n",
    "                    early_stopping_rounds=100, verbose_eval=100) \n",
    "preds4 = model4.predict(X_dev)\n",
    "print('Accuracy: ', roc_auc_score(y_valid, preds4))\n",
    "print('Prediction for lgb 2 completed.', time()-start, ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds3 = model3.predict(X_test)\n",
    "preds4 = model4.predict(X_test)\n",
    "preds = (0.5*preds3)+(0.5*preds4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'id': test_id, 'is_click': preds})\n",
    "out.head()\n",
    "out.to_csv(\"pred4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773858, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
