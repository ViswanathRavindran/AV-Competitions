{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,HashingVectorizer, TfidfTransformer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import multiprocessing as mp\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import gc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['JOBLIB_START_METHOD'] = 'forkserver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "path = '/Users/Vishy/Files/AVDatahack/LOM/Data/'\n",
    "# train = pd.read_csv(path+'train.csv',)\n",
    "# test = pd.read_csv(path+'test.csv')\n",
    "# camp = pd.read_csv(path+'campaign_data.csv')\n",
    "\n",
    "# print('New variable creations')\n",
    "# train['hour'] = pd.to_datetime(train.send_date).dt.hour.astype('uint8')\n",
    "# train['day'] = pd.to_datetime(train.send_date).dt.day.astype('uint8')\n",
    "# train['dow'] = pd.to_datetime(train.send_date).dt.dayofweek.astype('uint8')\n",
    "\n",
    "# test['hour'] = pd.to_datetime(test.send_date).dt.hour.astype('uint8')\n",
    "# test['day'] = pd.to_datetime(test.send_date).dt.day.astype('uint8')\n",
    "# test['dow'] = pd.to_datetime(test.send_date).dt.dayofweek.astype('uint8')\n",
    "\n",
    "# camp['sub_tokens'] = camp['subject'].map(tokenizer)\n",
    "# camp['email_tokens'] = camp['email_body'].map(tokenizer)\n",
    "# train_df = pd.merge(train, camp, on='campaign_id')\n",
    "# test_df = pd.merge(test, camp, on='campaign_id')\n",
    "\n",
    "train_df = pd.read_csv(path+'train_df.csv',nrows=20000,parse_dates=['send_date'])\n",
    "#test_df = pd.read_csv(path+'test_df.csv',nrows=100000,parse_dates=['send_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming of the new date variables created\n",
      "Time taken for renaming date variable:  0.33565282821655273  secs\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "print('Renaming of the new date variables created')\n",
    "train_df['dow'] = train_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "train_df['hour'] = train_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "train_df['day'] = train_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)\n",
    "\n",
    "test_df['dow'] = test_df['dow'].map({0:'MON', 1:'TUE', 2:'WED', 3:'THR',4: 'FRI', 5: 'SAT', 6: 'SUN'} ).astype(str)\n",
    "test_df['hour'] = test_df['hour'].map({0:'AM1', 1:'AM1', 2:'AM1', 3:'AM2', 4:'AM2', 5:'AM2', 6:'AM3', 7:'AM3', 8:'AM3',\n",
    "                                         9:'AM4', 10:'AM4', 11:'AM4', 12:'PM1', 13:'PM1', 14:'PM1', 15:'PM2', 16:'PM2',\n",
    "                                         17:'PM2', 18:'PM3', 19:'PM3', 20:'PM3', 21:'PM4', 22:'PM4', 23:'PM4'}).astype(str)\n",
    "test_df['day'] = test_df['day'].map({1:'VEAR', 2:'VEAR', 3:'VEAR', 4:'VEAR', 5:'VEAR', 6:'EAR', 7:'EAR', 8:'EAR',\n",
    "                                       9:'EAR', 10:'EAR', 11:'MID', 12:'MID', 13:'MID', 14:'MID', 15:'MID', 16:'VMID',\n",
    "                                       17:'VMID', 18:'VMID', 19:'VMID', 20:'VMID', 21:'LAT', 22:'LAT', 23:'LAT', \n",
    "                                       24:'LAT', 25:'LAT', 26:'VLAT', 27:'VLAT', 28:'VLAT', 29:'VLAT', 30:'VLAT',\n",
    "                                       31:'VLAT'}).astype(str)\n",
    "\n",
    "\n",
    "print('Time taken for renaming date variable: ', time()-start, ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ntraindf = train_df[['user_id','campaign_id','is_open','is_click','hour','day','dow','communication_type']]\n",
    "#ntraindf.to_excel('train.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ntraindf.user_id.nunique()\n",
    "#del ntraindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'campaign_id', 'send_date', 'is_open', 'is_click',\n",
       "       'hour', 'day', 'dow', 'communication_type', 'total_links',\n",
       "       'no_of_internal_links', 'no_of_images', 'no_of_sections', 'email_body',\n",
       "       'subject', 'email_url', 'sub_tokens', 'email_tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by next campaign for train complete\n",
      "Group by next campaign for test complete\n"
     ]
    }
   ],
   "source": [
    "# Group by next campaign\n",
    "GROUP_BY_NEXT_CAMP = [\n",
    "    {'groupby': ['user_id']},\n",
    "    {'groupby': ['user_id', 'communication_type']},\n",
    "    {'groupby': ['user_id', 'day']},\n",
    "    {'groupby': ['user_id', 'dow']},\n",
    "]\n",
    "\n",
    "for spec in GROUP_BY_NEXT_CAMP:\n",
    "    new_feature = '{}_nxt'.format('_'.join(spec['groupby']))    \n",
    "    all_features = spec['groupby'] + ['send_date']\n",
    "    train_df[new_feature] = train_df[all_features].groupby(spec['groupby']).send_date.transform(lambda x: x.diff().shift(-1)).dt.seconds\n",
    "print('Group by next campaign for train complete')\n",
    "    \n",
    "for spec in GROUP_BY_NEXT_CAMP:\n",
    "    new_feature = '{}_nxt'.format('_'.join(spec['groupby']))    \n",
    "    all_features = spec['groupby'] + ['send_date']\n",
    "    test_df[new_feature] = test_df[all_features].groupby(spec['groupby']).send_date.transform(lambda x: x.diff().shift(-1)).dt.seconds\n",
    "print('Group by next campaign for test complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for new variables in train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for new variables in train data\")\n",
    "train_df['user_id_nxt'] = train_df['user_id_nxt'].fillna(0).astype('uint16')\n",
    "train_df['user_id_communication_type_nxt'] = train_df['user_id_communication_type_nxt'].fillna(0).astype('uint16')\n",
    "train_df['user_id_day_nxt'] = train_df['user_id_day_nxt'].fillna(0).astype('uint16')\n",
    "train_df['user_id_dow_nxt'] = train_df['user_id_dow_nxt'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for new variables in train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for new variables in train data\")\n",
    "test_df['user_id_nxt'] = test_df['user_id_nxt'].fillna(0).astype('uint16')\n",
    "test_df['user_id_communication_type_nxt'] = test_df['user_id_communication_type_nxt'].fillna(0).astype('uint16')\n",
    "test_df['user_id_day_nxt'] = test_df['user_id_day_nxt'].fillna(0).astype('uint16')\n",
    "test_df['user_id_dow_nxt'] = test_df['user_id_dow_nxt'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by previous and next campaign for train complete\n",
      "Group by previous and next campaign for test complete\n"
     ]
    }
   ],
   "source": [
    "# Number of prev ang next campaign\n",
    "HISTORY_CAMP = {\n",
    "    'typ1': ['user_id', 'communication_type'],\n",
    "    'typ2': ['user_id', 'communication_type', 'dow'],\n",
    "    'typ3': ['user_id', 'communication_type', 'day'],\n",
    "}\n",
    "\n",
    "for fname, fset in HISTORY_CAMP.items():\n",
    "    train_df['prev_'+fname] = train_df.groupby(fset).cumcount().rename('prev_'+fname)\n",
    "    train_df['futr_'+fname] = train_df.iloc[::-1].groupby(fset).cumcount().rename('futr_'+fname).iloc[::-1]\n",
    "print('Group by previous and next campaign for train complete')\n",
    "    \n",
    "for fname, fset in HISTORY_CAMP.items():\n",
    "    test_df['prev_'+fname] = test_df.groupby(fset).cumcount().rename('prev_'+fname)\n",
    "    test_df['futr_'+fname] = test_df.iloc[::-1].groupby(fset).cumcount().rename('futr_'+fname).iloc[::-1]\n",
    "print('Group by previous and next campaign for test complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for new variables in train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for new variables in train data\")\n",
    "train_df['prev_typ1'] = train_df['prev_typ1'].fillna(0).astype('uint16')\n",
    "train_df['prev_typ2'] = train_df['prev_typ2'].fillna(0).astype('uint16')\n",
    "train_df['prev_typ3'] = train_df['prev_typ3'].fillna(0).astype('uint16')\n",
    "train_df['futr_typ1'] = train_df['futr_typ1'].fillna(0).astype('uint16')\n",
    "train_df['futr_typ2'] = train_df['futr_typ2'].fillna(0).astype('uint16')\n",
    "train_df['futr_typ3'] = train_df['futr_typ3'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for new variables in train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for new variables in train data\")\n",
    "test_df['prev_typ1'] = test_df['prev_typ1'].fillna(0).astype('uint16')\n",
    "test_df['prev_typ2'] = test_df['prev_typ2'].fillna(0).astype('uint16')\n",
    "test_df['prev_typ3'] = test_df['prev_typ3'].fillna(0).astype('uint16')\n",
    "test_df['futr_typ1'] = test_df['futr_typ1'].fillna(0).astype('uint16')\n",
    "test_df['futr_typ2'] = test_df['futr_typ2'].fillna(0).astype('uint16')\n",
    "test_df['futr_typ3'] = test_df['futr_typ3'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " User_ID Level Grouping variables creation for train_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping of variables # User_ID level\n",
    "print(' User_ID Level Grouping variables creation for train_data')\n",
    "\n",
    "# Dow\n",
    "gp = train_df[['user_id','dow','campaign_id']].groupby(by=['user_id','dow'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'udowcnt'})\n",
    "train_df = train_df.merge(gp, on=['user_id','dow'], how='left')\n",
    "test_df = test_df.merge(gp, on=['user_id','dow'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "# Day\n",
    "gp = train_df[['user_id','day','campaign_id']].groupby(by=['user_id','day'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'udaycnt'})\n",
    "train_df = train_df.merge(gp, on=['user_id','day'], how='left')\n",
    "test_df = test_df.merge(gp, on=['user_id','day'], how='left')\n",
    "del gp\n",
    "gc.collect()\n",
    "\n",
    "# Hour\n",
    "gp = train_df[['user_id','hour','campaign_id']].groupby(by=['user_id','hour'])[['campaign_id']].count().reset_index().rename(index=str, columns={'campaign_id': 'uhourcnt'})\n",
    "train_df = train_df.merge(gp, on=['user_id','hour'], how='left')\n",
    "test_df = test_df.merge(gp, on=['user_id','hour'], how='left')\n",
    "del gp\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for variables train data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for variables train data\")\n",
    "train_df['udowcnt'] = train_df['udowcnt'].fillna(0).astype('uint16')\n",
    "train_df['udaycnt'] = train_df['udaycnt'].fillna(0).astype('uint16')\n",
    "train_df['uhourcnt'] = train_df['uhourcnt'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data types for variables test data\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning data types for variables test data\")\n",
    "test_df['udowcnt'] = test_df['udowcnt'].fillna(0).astype('uint16')\n",
    "test_df['udaycnt'] = test_df['udaycnt'].fillna(0).astype('uint16')\n",
    "test_df['uhourcnt'] = test_df['uhourcnt'].fillna(0).astype('uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'user_id', 'campaign_id', 'send_date', 'is_open', 'is_click',\n",
       "       'hour', 'day', 'dow', 'communication_type', 'total_links',\n",
       "       'no_of_internal_links', 'no_of_images', 'no_of_sections', 'email_body',\n",
       "       'subject', 'email_url', 'sub_tokens', 'email_tokens', 'user_id_nxt',\n",
       "       'user_id_communication_type_nxt', 'user_id_day_nxt', 'user_id_dow_nxt',\n",
       "       'prev_typ1', 'futr_typ1', 'prev_typ2', 'futr_typ2', 'prev_typ3',\n",
       "       'futr_typ3', 'udowcnt', 'udaycnt', 'uhourcnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train data is  (180000, 32)  Test data is  (100000, 30)  Valid data is (20000, 32)\n",
      "Time taken for importing the data is:  1064.1751079559326  secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the data\n",
    "train_df, dev_df = train_test_split(train_df, random_state=42, test_size=0.10)\n",
    "\n",
    "print('Shape of the train data is ',train_df.shape,' Test data is ',test_df.shape,' Valid data is',dev_df.shape)\n",
    "print('Time taken for importing the data is: ', time()-start, ' secs')\n",
    "\n",
    "Y_train = train_df.is_click.values.reshape(-11, 1)\n",
    "Y_dev = dev_df.is_click.values.reshape(-1, 1)\n",
    "test_id = test_df['id'].values\n",
    "del test_df['id'],train_df['is_open'],train_df['is_click'],dev_df['is_open'],dev_df['is_click']\n",
    "\n",
    "n_trains = train_df.shape[0]\n",
    "n_devs = dev_df.shape[0]\n",
    "n_test = test_df.shape[0]\n",
    "\n",
    "full_df = pd.concat([train_df,dev_df,test_df])\n",
    "del train_df,dev_df,test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing categorical data...\n",
      "Time taken for processing categorical is: 2.05899715423584\n"
     ]
    }
   ],
   "source": [
    "# Converting the Categorical data\n",
    "print(\"Processing categorical data...\")\n",
    "start = time()\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(full_df.user_id)\n",
    "full_df.user_id = le.transform(full_df.user_id)\n",
    "\n",
    "le.fit(full_df.dow)\n",
    "full_df.dow = le.transform(full_df.dow)\n",
    "\n",
    "le.fit(full_df.hour)\n",
    "full_df.hour = le.transform(full_df.hour)\n",
    "\n",
    "le.fit(full_df.day)\n",
    "full_df.day = le.transform(full_df.day)\n",
    "\n",
    "le.fit(full_df.communication_type)\n",
    "full_df.communication_type = le.transform(full_df.communication_type)\n",
    "\n",
    "del le\n",
    "print('Time taken for processing categorical is:', time()-start )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming text data to sequences...\n",
      "Fitting tokenizer...\n",
      "Transforming text to sequences...\n",
      "Time taken for Transforming text data to sequences is: 274.07536911964417\n"
     ]
    }
   ],
   "source": [
    "# Transform text data into sequences\n",
    "print(\"Transforming text data to sequences...\")\n",
    "start = time()\n",
    "raw_text = np.hstack([full_df.subject.str.lower(), full_df.email_body.str.lower()])\n",
    "\n",
    "print(\"Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "\n",
    "print(\"Transforming text to sequences...\")\n",
    "full_df['seq_subject'] = tok_raw.texts_to_sequences(full_df.subject.str.lower())\n",
    "full_df['seq_email'] = tok_raw.texts_to_sequences(full_df.email_body.str.lower())\n",
    "\n",
    "del raw_text\n",
    "#del tok_raw\n",
    "print('Time taken for Transforming text data to sequences is:', time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300000 entries, 38762 to 99999\n",
      "Data columns (total 32 columns):\n",
      "campaign_id                       300000 non-null int64\n",
      "communication_type                300000 non-null int64\n",
      "day                               300000 non-null int64\n",
      "dow                               300000 non-null int64\n",
      "email_body                        300000 non-null object\n",
      "email_tokens                      300000 non-null object\n",
      "email_url                         300000 non-null object\n",
      "futr_typ1                         300000 non-null uint16\n",
      "futr_typ2                         300000 non-null uint16\n",
      "futr_typ3                         300000 non-null uint16\n",
      "hour                              300000 non-null int64\n",
      "id                                200000 non-null object\n",
      "no_of_images                      300000 non-null int64\n",
      "no_of_internal_links              300000 non-null int64\n",
      "no_of_sections                    300000 non-null int64\n",
      "prev_typ1                         300000 non-null uint16\n",
      "prev_typ2                         300000 non-null uint16\n",
      "prev_typ3                         300000 non-null uint16\n",
      "send_date                         300000 non-null datetime64[ns]\n",
      "sub_tokens                        300000 non-null object\n",
      "subject                           300000 non-null object\n",
      "total_links                       300000 non-null int64\n",
      "udaycnt                           300000 non-null uint16\n",
      "udowcnt                           300000 non-null uint16\n",
      "uhourcnt                          300000 non-null uint16\n",
      "user_id                           300000 non-null int64\n",
      "user_id_communication_type_nxt    300000 non-null uint16\n",
      "user_id_day_nxt                   300000 non-null uint16\n",
      "user_id_dow_nxt                   300000 non-null uint16\n",
      "user_id_nxt                       300000 non-null uint16\n",
      "seq_subject                       300000 non-null object\n",
      "seq_email                         300000 non-null object\n",
      "dtypes: datetime64[ns](1), int64(10), object(8), uint16(13)\n",
      "memory usage: 53.2+ MB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants to use when define RNN model\n",
    "MAX_SUB_SEQ = 20\n",
    "MAX_EMAIL_SEQ = 200\n",
    "MAX_TEXT = np.max([np.max(full_df.seq_subject.max()), np.max(full_df.seq_email.max()),]) + 4\n",
    "MAX_USER = np.max(full_df.user_id.max()) + 1\n",
    "MAX_DOW = np.max(full_df.dow.max()) + 1\n",
    "MAX_HOUR = np.max(full_df.hour.max()) + 1\n",
    "MAX_DAY = np.max(full_df.day.max()) + 1\n",
    "MAX_COM = np.max(full_df.communication_type.max()) + 1\n",
    "max_udowcnt = np.max(full_df.udowcnt.max())+1\n",
    "max_udaycnt = np.max(full_df.udaycnt.max())+1\n",
    "max_uhourcnt = np.max(full_df.uhourcnt.max())+1\n",
    "max_user_id_nxt = np.max(full_df.user_id_nxt.max())+1\n",
    "max_user_id_communication_type_nxt = np.max(full_df.user_id_communication_type_nxt.max())+1\n",
    "max_user_id_day_nxt = np.max(full_df.user_id_day_nxt.max())+1\n",
    "max_user_id_dow_nxt = np.max(full_df.user_id_dow_nxt.max())+1\n",
    "max_prev_typ1 = np.max(full_df.prev_typ1.max())+1\n",
    "max_prev_typ2 = np.max(full_df.prev_typ2.max())+1\n",
    "max_prev_typ3 = np.max(full_df.prev_typ3.max())+1\n",
    "max_futr_typ1 = np.max(full_df.futr_typ1.max())+1\n",
    "max_futr_typ2 = np.max(full_df.futr_typ2.max())+1\n",
    "max_futr_typ3 = np.max(full_df.futr_typ3.max())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for the RNN\n",
    "\n",
    "def get_keras_data(df):\n",
    "    X = {\n",
    "        'user_id': np.array(df.user_id),\n",
    "        'subject': pad_sequences(df.seq_subject, maxlen=MAX_SUB_SEQ),\n",
    "        'email': pad_sequences(df.seq_email, maxlen=MAX_EMAIL_SEQ),\n",
    "        'dow': np.array(df.dow),\n",
    "        'hour': np.array(df.hour),\n",
    "        'day': np.array(df.day),\n",
    "        'communication_type': np.array(df.communication_type),\n",
    "        'udowcnt': np.array(df.udowcnt),\n",
    "        'udaycnt': np.array(df.udaycnt),\n",
    "        'uhourcnt': np.array(df.uhourcnt),\n",
    "        'user_id_nxt': np.array(df.user_id_nxt),\n",
    "        'user_id_communication_type_nxt': np.array(df.user_id_communication_type_nxt),\n",
    "        'user_id_day_nxt': np.array(df.user_id_day_nxt),\n",
    "        'user_id_dow_nxt': np.array(df.user_id_dow_nxt),\n",
    "        'prev_typ1': np.array(df.prev_typ1),\n",
    "        'prev_typ2': np.array(df.prev_typ2),\n",
    "        'prev_typ3': np.array(df.prev_typ3),\n",
    "        'futr_typ1': np.array(df.futr_typ1),\n",
    "        'futr_typ2': np.array(df.futr_typ2),\n",
    "        'futr_typ3': np.array(df.futr_typ3),\n",
    "    }\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate data for the RNN...\n",
      "Time taken for Generate data for the RNN is: 10.397393941879272\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate data for the RNN...\")\n",
    "start = time()\n",
    "train = full_df[:n_trains]\n",
    "dev = full_df[n_trains:n_trains+n_devs]\n",
    "test = full_df[n_trains+n_devs:]\n",
    "\n",
    "X_train = get_keras_data(train)\n",
    "X_dev = get_keras_data(dev)\n",
    "X_test = get_keras_data(test)\n",
    "print('Time taken for Generate data for the RNN is:', time()-start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the RNN Model\n",
    "\n",
    "def new_rnn_model(lr=0.001, decay=0.0):    \n",
    "    # Inputs\n",
    "    user_id = Input(shape=[1], name=\"user_id\")\n",
    "    subject = Input(shape=[X_train[\"subject\"].shape[1]], name=\"subject\")\n",
    "    email = Input(shape=[X_train[\"email\"].shape[1]], name=\"email\")\n",
    "    dow = Input(shape=[1], name=\"dow\")\n",
    "    hour = Input(shape=[1], name=\"hour\")\n",
    "    day = Input(shape=[1], name=\"day\")\n",
    "    communication_type = Input(shape=[1], name=\"communication_type\")\n",
    "    udowcnt = Input(shape=[1], name=\"udowcnt\")\n",
    "    udaycnt = Input(shape=[1], name=\"udaycnt\")\n",
    "    uhourcnt = Input(shape=[1], name=\"uhourcnt\")\n",
    "    user_id_nxt = Input(shape=[1], name=\"user_id_nxt\")\n",
    "    user_id_communication_type_nxt = Input(shape=[1], name=\"user_id_communication_type_nxt\")\n",
    "    user_id_day_nxt = Input(shape=[1], name=\"user_id_day_nxt\")\n",
    "    user_id_dow_nxt = Input(shape=[1], name=\"user_id_dow_nxt\")\n",
    "    prev_typ1 = Input(shape=[1], name=\"prev_typ1\")\n",
    "    prev_typ2 = Input(shape=[1], name=\"prev_typ2\")\n",
    "    prev_typ3 = Input(shape=[1], name=\"prev_typ3\")\n",
    "    futr_typ1 = Input(shape=[1], name=\"futr_typ1\")\n",
    "    futr_typ2 = Input(shape=[1], name=\"futr_typ2\")\n",
    "    futr_typ3 = Input(shape=[1], name=\"futr_typ3\")\n",
    "\n",
    "    \n",
    "    # Embeddings layers\n",
    "    emb_user_id = Embedding(MAX_USER, 10)(user_id)\n",
    "    emb_subject = Embedding(MAX_TEXT, 20)(subject)\n",
    "    emb_email = Embedding(MAX_TEXT, 60)(email)\n",
    "    emb_dow = Embedding(MAX_DOW, 10)(dow)\n",
    "    emb_hour = Embedding(MAX_HOUR, 10)(hour)\n",
    "    emb_day = Embedding(MAX_DAY, 5)(day)\n",
    "    emb_communication_type = Embedding(MAX_COM, 5)(communication_type)    \n",
    "    emb_udowcnt = Embedding(max_udowcnt, 5)(udowcnt)\n",
    "    emb_udaycnt = Embedding(max_udaycnt, 5)(udaycnt)\n",
    "    emb_uhourcnt = Embedding(max_uhourcnt, 5)(uhourcnt)\n",
    "    emb_user_id_nxt = Embedding(max_user_id_nxt, 5)(user_id_nxt)\n",
    "    emb_user_id_communication_type_nxt = Embedding(max_user_id_communication_type_nxt, 5)(user_id_communication_type_nxt)\n",
    "    emb_user_id_day_nxt = Embedding(max_user_id_day_nxt, 5)(user_id_day_nxt)\n",
    "    emb_user_id_dow_nxt = Embedding(max_user_id_dow_nxt, 5)(user_id_dow_nxt)\n",
    "    emb_prev_typ1 = Embedding(max_prev_typ1, 5)(prev_typ1)\n",
    "    emb_prev_typ2 = Embedding(max_prev_typ2, 5)(prev_typ2)\n",
    "    emb_prev_typ3 = Embedding(max_prev_typ3, 5)(prev_typ3)\n",
    "    emb_futr_typ1 = Embedding(max_futr_typ1, 5)(futr_typ1)\n",
    "    emb_futr_typ2 = Embedding(max_futr_typ2, 5)(futr_typ2)\n",
    "    emb_futr_typ3 = Embedding(max_futr_typ3, 5)(futr_typ3)\n",
    "\n",
    "    \n",
    "    # rnn layers\n",
    "    rnn_layer1 = GRU(32) (emb_email)\n",
    "    rnn_layer2 = GRU(16) (emb_subject)\n",
    "\n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        Flatten()(emb_user_id),\n",
    "        Flatten()(emb_dow),\n",
    "        Flatten()(emb_hour),\n",
    "        Flatten()(emb_day),\n",
    "        Flatten()(emb_communication_type),\n",
    "        Flatten()(emb_udowcnt),\n",
    "        Flatten()(emb_udaycnt),\n",
    "        Flatten()(emb_uhourcnt),\n",
    "        Flatten()(emb_user_id_nxt),\n",
    "        Flatten()(emb_user_id_communication_type_nxt),\n",
    "        Flatten()(emb_user_id_day_nxt),\n",
    "        Flatten()(emb_user_id_dow_nxt),\n",
    "        Flatten()(emb_prev_typ1),\n",
    "        Flatten()(emb_prev_typ2),\n",
    "        Flatten()(emb_prev_typ3),\n",
    "        Flatten()(emb_futr_typ1),\n",
    "        Flatten()(emb_futr_typ2),\n",
    "        Flatten()(emb_futr_typ3),\n",
    "        rnn_layer1,\n",
    "        rnn_layer2\n",
    "    ])\n",
    "    \n",
    "    \"\"\"s_dout = SpatialDropout1D(0.2)(fe)\n",
    "    fl1 = Flatten()(s_dout)\n",
    "    conv = Conv1D(100, kernel_size=4, strides=1, padding='same')(s_dout)\n",
    "    fl2 = Flatten()(conv)\n",
    "    concat = concatenate([(fl1), (fl2)])\n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(concat))\n",
    "    x = Dropout(0.2)(Dense(dense_n,activation='relu')(x))\n",
    "    outp = Dense(1,activation='sigmoid')(x)\"\"\"\n",
    "\n",
    "    main_l = Dropout(0.2)(main_l)\n",
    "    main_l = Dense(512)(main_l)\n",
    "    main_l = Activation('relu')(main_l)\n",
    "\n",
    "    main_l = Dropout(0.2)(main_l)\n",
    "    main_l = Dense(256)(main_l)\n",
    "    main_l = Activation('relu')(main_l)\n",
    "    \n",
    "    main_l = Dropout(0.2)(main_l)\n",
    "    main_l = Dense(64)(main_l)\n",
    "    main_l = Activation('relu')(main_l)\n",
    "    \n",
    "    # the output layer.\n",
    "    output = Dense(1, activation=\"sigmoid\") (main_l)\n",
    "    model = Model([user_id, subject, email, dow , hour, day, communication_type, udowcnt, udaycnt, uhourcnt,\n",
    "                   user_id_nxt, user_id_communication_type_nxt, user_id_day_nxt, user_id_dow_nxt,\n",
    "                   prev_typ1, futr_typ1, prev_typ2, futr_typ2, prev_typ3, futr_typ3], output) #udowclk,udayclk,uhourclk\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = new_rnn_model()\n",
    "#model.summary()\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RNN...\n",
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/3\n",
      " - 235s - loss: 0.0793 - acc: 0.9868 - val_loss: 0.0706 - val_acc: 0.9867\n",
      "Epoch 2/3\n",
      " - 220s - loss: 0.0411 - acc: 0.9869 - val_loss: 0.1001 - val_acc: 0.9867\n",
      "Epoch 3/3\n",
      " - 223s - loss: 0.0172 - acc: 0.9910 - val_loss: 0.2074 - val_acc: 0.9349\n",
      "Time taken for Training the RNN: 681.9215769767761\n",
      "Evaluating the model on validation data...\n",
      "ROC-AUC error: 0.5623234507673868\n"
     ]
    }
   ],
   "source": [
    "# Learning for the RNN\n",
    "print(\"Training the RNN...\")\n",
    "start = time()\n",
    "# Set hyper parameters for the model.\n",
    "BATCH_SIZE = 1024\n",
    "epochs = 3\n",
    "\n",
    "# Calculate learning rate decay.\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(n_trains / BATCH_SIZE) * epochs\n",
    "lr_init, lr_fin = 0.007, 0.0005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "rnn_model = new_rnn_model(lr=lr_init, decay=lr_decay)\n",
    "\n",
    "rnn_model.fit(X_train, Y_train, epochs=epochs, \n",
    "            batch_size=BATCH_SIZE,validation_data=(X_dev, Y_dev), verbose=2)\n",
    "print('Time taken for Training the RNN:', time()-start )\n",
    "\n",
    "print(\"Evaluating the model on validation data...\")\n",
    "Y_dev_preds_rnn = rnn_model.predict(X_dev, batch_size=BATCH_SIZE)\n",
    "print(\"ROC-AUC error:\", roc_auc_score(Y_dev, Y_dev_preds_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 396us/step\n",
      "Prediction for RNN complete\n"
     ]
    }
   ],
   "source": [
    "rnn_preds = rnn_model.predict(X_dev, batch_size=BATCH_SIZE, verbose=1)\n",
    "print ('Prediction for RNN complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'id': test_id, 'is_click': rnn_preds.reshape(-1)})\n",
    "out.head()\n",
    "out.to_csv(\"pred4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
